{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "PyTorch 是基于 Python 的科学计算包，主要有两大特色：\n",
    "\n",
    "* 用于多维数组的计算，可以作为 NumPy 的替代品。PyTorch 具有 GPU 加速的优势\n",
    "* 提供了许多深度学习方面的开发工具，如神经网络库、优化算法、自动微分等\n",
    "\n",
    "# Tensor\n",
    "\n",
    "PyTorch 中主要的计算对象是 Tensor，就如同 NumPy 中的 ndarray 一样。不过所有基于 Tensor 的计算，都拥有 GPU 加速带来的性能优势。Tensor 支持丰富的计算操作，同时 PyTorch 会为这些操作维护计算图，以便实现自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 创建**\n",
    "\n",
    "如果之前使用过 NumPy，那应该对下面这些创建 Tensor 的接口不会陌生，它们跟 NumPy 中创建 ndarray 的接口名称一样且功能类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[7.7885e+34, 6.2618e+22, 4.7428e+30, 4.6172e+24, 1.4607e-19],\n",
      "        [1.1578e+27, 7.8981e+34, 1.2121e+04, 7.1846e+22, 6.7437e+22],\n",
      "        [4.2326e+21, 1.6931e+22, 0.0000e+00, 0.0000e+00, 2.1019e-44]])\n",
      "tensor([[7.6059e+31, 1.7860e+25, 2.8930e+12, 7.5338e+28, 1.8037e+28],\n",
      "        [3.4740e-12, 1.4583e-19, 1.1578e+27, 1.1362e+30, 7.1547e+22],\n",
      "        [4.5828e+30, 1.2121e+04, 7.1846e+22, 5.7453e-44, 2.1019e-44]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[-1., -1., -1.],\n",
      "        [-1., -1., -1.]])\n",
      "tensor([[-2, -2, -2],\n",
      "        [-2, -2, -2]], dtype=torch.int32)\n",
      "tensor([[-0.5801,  0.6799, -0.1544,  0.3945,  1.6921],\n",
      "        [ 0.8022,  0.6898,  0.4960, -0.7121,  1.4779],\n",
      "        [-0.7273,  0.5356, -1.2367,  0.3584,  0.3766]])\n",
      "tensor([[-0.7955,  0.3765, -2.1934,  0.2224, -1.9504],\n",
      "        [-0.7725, -0.4679, -1.2800,  0.4111,  0.9966],\n",
      "        [-0.8036, -0.4600,  1.5876,  1.7226, -0.1928]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.tensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "print(x.size())\n",
    "\n",
    "x = th.empty(3, 5)\n",
    "print(x)\n",
    "\n",
    "y = th.empty_like(x)\n",
    "print(y)\n",
    "\n",
    "x = th.zeros(3, 5)\n",
    "print(x)\n",
    "\n",
    "y = th.zeros_like(x)\n",
    "print(y)\n",
    "\n",
    "x = th.ones(3, 5, dtype=th.long)\n",
    "print(x)\n",
    "\n",
    "y = th.ones_like(x)\n",
    "print(y)\n",
    "\n",
    "x = th.eye(4)\n",
    "print(x)\n",
    "\n",
    "x = th.eye(4, 4)\n",
    "print(x)\n",
    "\n",
    "x = th.full((2, 3), -1.0)\n",
    "print(x)\n",
    "\n",
    "y = th.full_like(x, -2, dtype=th.int)\n",
    "print(y)\n",
    "\n",
    "x = th.randn(3, 5)\n",
    "print(x)\n",
    "\n",
    "y = th.randn_like(x, dtype=th.double)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意，Tensor 的维度可以是零维的（也即是标量），如：`torch.tensor(3.14)` 就是一个零维 Tensor，不要将它跟 `torch.tensor([3.14])` 相混淆，后者是一个一维 Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400) tensor([3.1400])\n",
      "torch.Size([]) torch.Size([1])\n",
      "3.140000104904175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(3.14)\n",
    "b = torch.tensor([3.14])\n",
    "print(a, b)\n",
    "print(a.size(), b.size())\n",
    "print(a.item())  # get the value of scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 运算**\n",
    "\n",
    "Torch 支持丰富的 Tensor 运算操作，并且这些运算都有两大特点：\n",
    "\n",
    "* 支持 CUDA 加速\n",
    "* 支持自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.rand(2, 2)\n",
    "y = th.rand(2, 2)\n",
    "z = th.empty(2, 2)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "z = th.add(x, y)\n",
    "print(z)\n",
    "\n",
    "th.add(x, y, out=z)\n",
    "print(z)\n",
    "\n",
    "y.add_(x)  # add x to y in-place\n",
    "print(y)\n",
    "\n",
    "print(z)\n",
    "z.zero_()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 索引**\n",
    "\n",
    "索引语法跟 NumPy 中索引 ndarray 是类似的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(x[0, 0])\n",
    "print(x[0, :])\n",
    "print(x[0:1, :])\n",
    "print(x[0:1, 0:1])\n",
    "print(x[0:1, 0:1].item())  # if the tensor just have one number, item() can return its scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 重塑**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"change shape\")\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size())\n",
    "\n",
    "print(\"add new axis\")\n",
    "y = x.view(-1, 4, 4)\n",
    "z = x.view(1, 4, 4)\n",
    "print(x.size(), y.size(), z.size())\n",
    "a = x.unsqueeze(0)\n",
    "b = x.unsqueeze(1)\n",
    "print(x.size(), a.size(), b.size())\n",
    "a = x[None]\n",
    "b = x[:, None]\n",
    "print(x.size(), a.size(), b.size())\n",
    "\n",
    "print(\"remove axis\")\n",
    "y = x.view(16)\n",
    "z = x[:,None]\n",
    "a = z.squeeze()  # remove all the dimensions with size 1, removed\n",
    "b = z.squeeze(1)  # remove 1-dim if its size is 1, removed\n",
    "c = z.squeeze(0)  # remove 0-dim if its size is 1, cannot remove\n",
    "print(x.size(), y.size(), z.size(), a.size(), b.size(), c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**处理 NumPy 数组**\n",
    "\n",
    "Torcch 中的 Tensor 和 NumPy 中的 ndarray，可以实现相互转换。并且转换后它们共享数据储存，改变一个对象，另外一个对象也会被改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "print(\"Torch Tensor to NumPy Array\")\n",
    "\n",
    "a = torch.ones(3)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a.add_(1)  # change a(tensor), will change b(ndarray)\n",
    "print(b)\n",
    "b += 1\n",
    "print(a)  # change b(ndarray), will change a(tensor)\n",
    "\n",
    "\n",
    "print(\"NumPy Array to Torch Tensor\")\n",
    "c = numpy.ones(3)\n",
    "d = torch.from_numpy(c)\n",
    "print(c, d)\n",
    "\n",
    "numpy.add(c, 1, out=c)  # change c(ndarray), will change d(tensor)\n",
    "print(d)\n",
    "d.add_(1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 和 CUDA**\n",
    "\n",
    "Torch 中的 Tensor 支持利用 CUDA 进行计算，可以自由的在 CUDA 和 CPU 计算设备之间进行切换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "else:\n",
    "    print(\"CUDA isn't available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子可以知道，我们既可以在创建 Tensor 对象时，通过参数 `device` 来为其指定计算设备，也可以在创建完成以后通过方法 `to(device)`来切换计算设备。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor 对象深入\n",
    "\n",
    "经过上面的简单介绍后，让我们深入了解一下 Tensor 对象。\n",
    "\n",
    "## Tensor 创建\n",
    "\n",
    "**torch.tensor**\n",
    "\n",
    "类似于 NumPy 中 `numpy.array` 的作用，通过复制其它数据对象来创建。\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.tensor(data, dtype=None, device=None, requires_grad=False)\n",
    "\n",
    "* `data`：复制该对象的数据来创建 Tensor\n",
    "* `dtype`：指定 Tensor 的数据类型\n",
    "* `device`：指定计算设备\n",
    "* `requires_grad`：是否为该变量开启自动微分\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.tensor([]))  # create a empty tensor with size of (0,), one-dim\n",
    "\n",
    "print(torch.tensor([1]))  # create a tensor with size of (1,), one-dim\n",
    "\n",
    "print(torch.tensor(1.0))  # create a scalar, zero-dim\n",
    "\n",
    "print(torch.tensor([[1, 2], [3, 4]]))  # auto infer dtype int\n",
    "\n",
    "print(torch.tensor([[1.0, 2.0], [3.0, 4.0]]))  # auto infer dtype float\n",
    "\n",
    "l = [[1, 2], [3, 4]]\n",
    "t = torch.tensor(l)\n",
    "l[0][0] = -1\n",
    "print(t)  # don't change the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意区分维度数量和维度尺寸的区别：一个 Tensor 有可能零个或多个维度，每个维度有各自的尺寸（大于等于零）。因此 `torch.tensor(1.0)` 和 `torch.tensor([1.0])` 显然是不同的，前者是零维的，后者是一维的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.arange**\n",
    "\n",
    "类似于 NumPy 中 `numpy.arange` 的作用，快速创建一维序列。\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "注：序列范围是 `[start, end)`，不包括 `end`。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "\n",
    "print(a)  # 0...9\n",
    "\n",
    "b = torch.arange(12).view(3, 4)\n",
    "print(b)\n",
    "\n",
    "c = torch.arange(0, 12, 3)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于浮点数精度的有限性，`torch.arange` 跟 NumPy 中的 `numpy.arange` 一样都无法创建确定长度的序列，如果想要创建确定长度的序列，就需要用到 `numpy.linspace` 和 `torch.linspace` 了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.linspace**\n",
    "\n",
    "类似于 Numpy 中的 `numpy.linspace`，用来创建指定长度的序列。\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "\n",
    "注：序列的范围是 `[start, end]`；其中的参数 `steps` 指定期望返回的序列长度。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(1, 10, 3)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有作用类似的接口 `torch.logspace`，不过它的序列范围是 `[pow(10, start), pow(10, end)]`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机采样**\n",
    "\n",
    "利用各种概率分布，通过采样来创建 Tensor。\n",
    "\n",
    "常用采样接口如下：\n",
    "\n",
    "* `torch.randn(sizes, ...)`：从标准正态分布进行采样\n",
    "* `torch.randn_like(input, ...)`：从标准正态分布进行采样\n",
    "* `torch.normal(mean, std, ...)`：从一般正态分布进行采样\n",
    "* `torch.rand(sizes, ...)`：从 `[0, 1)` 进行均匀采样\n",
    "* `torch.rand_like(input, ...)`：从 `[0, 1)` 进行均匀采样\n",
    "* `torch.randint(low=0, high, sizes, ...)`：从 `[low, high)`进行整数均匀采样\n",
    "* `torch.randint_like(input, low, high, ...)`：从 `[low, high)`进行整数均匀采样\n",
    "* `torch.randperm(n)`：返回 `[0, n-1]` 的随机排列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 运算操作\n",
    "\n",
    "Tensor 支持常见的各种向量、矩阵等数学运算操作，具体参见[文档](https://pytorch.org/docs/stable/torch.html#math-operations)。\n",
    "\n",
    "常用的运算和函数：\n",
    "\n",
    "**torch.clamp**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.clamp(input, min, max, out=None)\n",
    " \n",
    "返回新的张量，若 `input` 张量中的元素小于 `min` 则取 `min`，大于 `max` 则取 `max`，其它元素跟 `input` 保持一致。\n",
    "\n",
    "备注：可以使用该函数实现 ReLU 激活函数。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)\n",
    "\n",
    "b = torch.clamp(a, -.9, .9)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(torch.clamp(a, min=0.0))\n",
    "print(torch.clamp(a, max=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.erf**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.erf(tensor, out=None)\n",
    "\n",
    "该函数在计算科学中有广泛的应用。\n",
    "\n",
    "备注：可以用来计算标准正态分布的累积函数以及 Probit 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.lerp**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.lerp(start, end, weight, out=None)\n",
    "\n",
    "返回值等于 `start + weight*(end-start)`，其中`weight` 是实数，`start` 和 `end` 是张量，该计算基于张量的元素进行。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10)\n",
    "b = torch.ones(10)\n",
    "w = 0.34\n",
    "\n",
    "c = torch.lerp(a, b, w)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.sigmoid**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.sigmoid(input, out=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.argmin 和 torch.argmax**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.argmin(input, dim=None, keepdim=False)\n",
    "    torch.argmax(input, dim=None, keepdim=False)\n",
    "\n",
    "返回张量 `input` 某个维度上，最小值/最大值所在位置的索引值。同 NumPy 一样，这里要把 `dim` 指定的维度，想象成要坍缩的方向。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 4)\n",
    "\n",
    "print(a)\n",
    "print(torch.argmax(a))  # 没有指定 dim 时，返回整个张量中最值的索引位置\n",
    "\n",
    "print(torch.argmin(a, 1))\n",
    "print(torch.argmin(a, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.min 和 torch.max**\n",
    "\n",
    "该接口用多种调用方式：\n",
    "\n",
    "1. `torch.min(input)`\n",
    "    返回张量 `input` 中的最小值。\n",
    "2. `torch.min(input, dim)`\n",
    "    返回张量 `input` 中的某个维度的最小值及其索引值。\n",
    "3. `torch.min(input, other)`\n",
    "    基于元素的操作，返回两个张量 `input` 和 `other` 中元素较小的那个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.sum 和 torch.prod**\n",
    "\n",
    "它们都有两种调用方式：\n",
    "\n",
    "    (input, dtype=None) \n",
    "    (input, dim, keepdim=False, dtype=None)\n",
    "\n",
    "分别用来求整个张量的和/积，或某个维度的和/积。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)\n",
    "\n",
    "print(a)\n",
    "print(torch.sum(a))\n",
    "print(torch.sum(a, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.cumprod 和 torch.cumsum**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.cumprod(input, dim, dtype=None)\n",
    "    torch.cumsum(input, dim, dtype=None)\n",
    "\n",
    "对张量 `input` 某个维度进行累乘/累加。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(1, 10)\n",
    "\n",
    "print(torch.cumprod(a, dim=0))\n",
    "\n",
    "print(torch.cumsum(a, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.norm**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.norm(input, p=2)\n",
    "    torch.norm(input, p, dim, keepdim=False, out=None)\n",
    "\n",
    "计算张量的模，通过参数 `p` 来指定计算几阶模。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 4) * -1\n",
    "\n",
    "print(a)\n",
    "print(torch.norm(a))\n",
    "print(torch.norm(a, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.dist**\n",
    "\n",
    "接口如下所示：\n",
    "\n",
    "    torch.dist(input, other, p=2)\n",
    "\n",
    "计算张量 `input - other` 的模，其中参数 `p` 指定计算几阶模。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "\n",
    "print(torch.dist(a, b))\n",
    "print(torch.sqrt(torch.tensor(2*3.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**统计量**\n",
    "\n",
    "有关统计的接口：\n",
    "\n",
    "* `torch.mean`\n",
    "* `torch.std`\n",
    "* `torch.var`\n",
    "* `torch.mode`\n",
    "* `torch.median`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**比较运算**\n",
    "\n",
    "相关接口：\n",
    "\n",
    "* `torch.ne`\n",
    "* `torch.eq`\n",
    "* `torch.gt`\n",
    "* `torch.ge`\n",
    "* `torch.lt`\n",
    "* `torch.le`\n",
    "* `torch.kthvalue`：第 k 个最小值\n",
    "* `torch.topk`：第 k 个最大值\n",
    "* `torch.sort`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**特殊张量**\n",
    "\n",
    "* `torch.isnan(input)`：是不是 NaN\n",
    "* `torch.isinf(input)`：是不是 +/-INF\n",
    "* `torch.isfinite(input)`：是不是 Finite\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.isnan(torch.tensor([1, float('nan'), 2]))\n",
    "print(a)\n",
    "\n",
    "b = torch.isinf(torch.Tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\n",
    "print(b)\n",
    "\n",
    "c = torch.isfinite(torch.Tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它\n",
    "\n",
    "**Tensor 连接**\n",
    "\n",
    "将若干个 Tensor 连接合并为一个新的 Tensor。\n",
    "\n",
    "接口：\n",
    "\n",
    "    torch.cat(seq, dim=0, out=None)\n",
    "\n",
    "其中 `dim` 指定连接的方向。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1520, -0.6978, -0.1785],\n",
      "        [-0.5567,  0.2475,  0.6820],\n",
      "        [-0.8564,  1.3369, -0.3055],\n",
      "        [ 0.3720,  1.9155,  0.9682],\n",
      "        [-0.4323,  1.5643, -1.9333],\n",
      "        [ 1.5893, -1.2790,  0.9977]])\n",
      "tensor([[ 0.1344,  2.2658, -0.2789, -0.8518,  0.3390,  1.8597,  1.6467,  0.0927,\n",
      "         -2.0039],\n",
      "        [ 0.1801, -0.1230, -0.1447,  0.0839, -0.4457, -1.5881,  0.2466, -0.0933,\n",
      "          0.2281],\n",
      "        [ 0.8701,  0.8677, -0.4961, -1.2346, -1.2981,  0.8937, -0.3769,  0.9773,\n",
      "          0.8147]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(4, 3)\n",
    "print(torch.cat((a, b)))  # cat by row\n",
    "\n",
    "c = torch.randn(3, 4)\n",
    "d = torch.randn(3, 5)  \n",
    "print(torch.cat((c, d), dim=1))  # cat by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 分割**\n",
    "\n",
    "将一个 Tensor 分割为若干个 Tensor。\n",
    "\n",
    "接口：\n",
    "\n",
    "    torch.chunk(tensor, chunks, dim=0)\n",
    "\n",
    "其中：`chunks` 指定块的数目，`dim` 指定分割方向，如果不能等分的话，最后一个块会比较小。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]]), tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]]))\n",
      "(tensor([[ 0,  1,  2],\n",
      "        [ 5,  6,  7],\n",
      "        [10, 11, 12],\n",
      "        [15, 16, 17]]), tensor([[ 3,  4],\n",
      "        [ 8,  9],\n",
      "        [13, 14],\n",
      "        [18, 19]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(20).view(4, 5)\n",
    "\n",
    "print(torch.chunk(a, 2))  # 按照行，分为两块，可以等分\n",
    "print(torch.chunk(a, 2, dim=1))  # 按照列，分为两块，不可等分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "自动微分技术是现代深度学习框架的一大特色，MXNet,TensorFlow 等框架都提供了自动微分工具，使得梯度的求解对于机器学习开发者变得透明。同样 Torch 也支持自动微分，通过子库 `autograd` 提供的自动微分技术，使得一切基于 Tensor 的运算操作都可以进行自动微分。\n",
    "\n",
    "有了自动微分的支持，机器学习的算法中涉及到导数/梯度的求解，都不需要开发者来关心具体的实现细节，开发框架会在背后自动完成计算。比如在开发神经网络模型时，只要定义了模型结构，就也意味着同时定义了梯度求解方法，后向传播的过程对开发者变得透明。\n",
    "\n",
    "Torch 中自动微分的实现是通过记录追踪计算对象 Tensor 和计算操作 Function 对象而构建计算图，有了计算图就有了完整的计算历史过程，进而可以实现自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**计算图简介**\n",
    "\n",
    "计算图是以变量 Tensor 为节点，以计算操作 Function 为边，而构成的有向无环图。\n",
    "\n",
    "在神经网络模型中，计算图中的叶节点一般是输入数据或者模型参数，根节点一般是损失函数值，其余节点是网络中隐含层等中间状态变量。当然，利用计算图实现自动微分，并不限于在神经网络模型中的应用，我们可以将这项技术应用到任何模型中\n",
    "\n",
    "计算图和自动微分的关系：\n",
    "\n",
    "* 计算图是自动微分的基础，因为计算图中记录了变量计算操作的历史信息。\n",
    "* 计算操作是双向的：forward 根据输入变量计算输出变量，backward 根据输出变量的梯度值来计算输入变量的梯度值。\n",
    "* 通过 forward 传播，可以计算模型的输出值。\n",
    "* 通过 backward 传播，可以计算所有输入变量的梯度值。由于该过程是根据计算图，并运用链式求导法则，自动完成的梯度计算，因此被称为自动微分。\n",
    "\n",
    "计算图对于 Torch 的使用者是透明的，主要体现在两方面：\n",
    "\n",
    "* 计算图的创建是透明的：基于 Tensor 的各种计算操作，其背后隐含了计算图的构建过程。\n",
    "* 计算图的销毁和管理也是透明的：为了高效的内存管理，Torch 会在合适的时机自动销毁计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用自动微分**\n",
    "\n",
    "Tensor 默认是不能自动微分的。要想使得某个 Tensor 可以进行自动微分，需要显式地进行声明，声明过后所有对该 Tensor 的运算操作都在背后隐含的构建了计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显式开启自动微分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = torch.ones(2, 2)\n",
    "print(y)\n",
    "y.requires_grad_(True)  # you can also use \"y.requires_grad = True\"\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐式开启自动微分：在计算图中所有直接定义的 Tensor 叫做叶节点变量，而所有通过计算操作生成的变量则叫做非叶节点变量，一般情况下非叶节点是否启动自动微分将由生成它的其它变量来决定，我们是不能直接设置非叶节点的 `requires_grad` 属性的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a + 1\n",
    "print(b.requires_grad)\n",
    "\n",
    "c = torch.ones(2, 2, requires_grad=True)\n",
    "d = c + a  # 参与运算的变量中任意一个开启了自动微分，则结果变量也会开启\n",
    "print(d.requires_grad)\n",
    "\n",
    "# 下面的代码将引发错误\n",
    "# d.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "较复杂的一种情况是，参与运算的变量有的开启了自动微分，有的没有开启："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2)\n",
    "b = torch.ones(2)\n",
    "\n",
    "c = a + b\n",
    "print(c.requires_grad)  # a 和 b 均没有开启，因此 c 也没有开启\n",
    "\n",
    "d = torch.ones(2, requires_grad=True)\n",
    "\n",
    "c = c + d\n",
    "print(c.requires_grad)  # 由于 d 开启了，因此 c 也开启"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开启自动微分后，我们就能够通过 Tensor 对象的 `grad_fn` 属性来引用生成它的计算操作对象，也即追溯计算历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "y = x + 1\n",
    "z = y ** 2\n",
    "a = z + 1\n",
    "b = a ** 2\n",
    "c = b.mean()\n",
    "\n",
    "fn = c.grad_fn\n",
    "while fn is not None and fn.next_functions:\n",
    "    print(fn)\n",
    "    fn = fn.next_functions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度的计算**\n",
    "\n",
    "当声明 Tensor 启用自动微分时，关于它的计算操作历史被保存在计算图中。在计算梯度时，将按照计算图中的历史记录和链式求导法则，来自动完成梯度的计算。\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a ** 3\n",
    "c = b + 3\n",
    "d = c * 2\n",
    "\n",
    "print('d=2*(3+pow(a, 3))')\n",
    "d.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上例中，后向传播的起点 `b` 是一个标量（零维 Tensor），`b.backward()` 的意思是：求计算图中其它变量，相对于该标量的导数值，也即求 d(d)/ d(a) 等。\n",
    "\n",
    "再看一个较复杂的例子，其中后向传播的起点变量不是标量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "try:\n",
    "    # y 不是标量，需要传递参数给 backward 函数，这里没有传递参数，所以会报错\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在解决这个问题之前，让我们先来看看，在非标量 Tensor 上调用 `backward()` 的意义：首先要明白，梯度的求解，总是相对于一个标量而言的，那么这里为什么又要在一个非标量上调用 `backward()`，并且传递一个参数？\n",
    "\n",
    "假设现在有一个标量 `l`，它是关于非标量 `y` 的函数值，而 `y` 各个元素又都是关于 `x` 的函数值，也即有 `l=l(y(x))`。我们想要计算 `x` 相对 `l` 的梯度值（`d(l)/d(x)`），如果现在我们已经知道了 `y` 相对于 `l` 的梯度值 `y_grad`（即 `d(l)/d(y)`），根据链式法则有：`d(l)/d(x) = (d(l)/d(y)) * (d(y)/d(x)) = y_grad * (d(y)/d(x))`，因此我们只要再求出 `d(y)/d(x)` 就可以得到 `x` 相对 `l` 的梯度值。\n",
    "\n",
    "而调用 `y.backward()` 正是用来求解 `d(y)/d(x)` 的，不过需要我们把 `y` 相对于标量 `l` 的梯度值作为参数传入，即有 `y.backward(y_grad)`，这样就可以通过后向传播求解 `x` 相对于 `l` 的梯度值了。（注：这里 `x` 可以是计算图中以 `y` 为根节点的任意后代节点）\n",
    "\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函数是基于标量的 backward\n",
    "def scalar_grad():\n",
    "    x = torch.ones(2, requires_grad=True)\n",
    "    y = x ** 2\n",
    "\n",
    "    l = torch.sum(y)  # l is a scalar\n",
    "    \n",
    "    # backward start from l\n",
    "    l.backward()\n",
    "\n",
    "    print('d(l) / d(y) = [1.0, 1.0]')\n",
    "    print('d(y) / d(x) = [2.0, 2.0]')\n",
    "    print('d(l) / d(x) = (d(l)/d(y)) * (d(y)/d(x)) = {}'.format(x.grad))\n",
    "\n",
    "\n",
    "# 此函数是基于非标量的 backward\n",
    "def non_scalar_grad():\n",
    "    x = torch.ones(2, requires_grad=True)\n",
    "    y = x ** 2  # y is a tensor 1x2\n",
    "\n",
    "    l = torch.sum(y)\n",
    "    \n",
    "    # backward from y\n",
    "    # 这里我们手动计算了 d(l)/d(y)，并将其作为参数输入 backward 函数\n",
    "    # 提示：传入的参数是 y 的梯度值，因此其形状必然跟 y 是一样的\n",
    "    y.backward(torch.tensor([1.0, 1.0]))\n",
    "    \n",
    "    print('d(l) / d(y) = [1.0, 1.0]')\n",
    "    print('d(y) / d(x) = [2.0, 2.0]')\n",
    "    print('d(l) / d(x) = (d(l)/d(y)) * (d(y)/d(x)) = {}'.format(x.grad))\n",
    "\n",
    "\n",
    "print('backward from a scalar tensor')\n",
    "scalar_grad()\n",
    "print('backward from a non-scalar tensor')\n",
    "non_scalar_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中间变量的梯度值**\n",
    "\n",
    "之前都是展示了叶节点变量的梯度值，也即程序中定义变量的梯度值，那么中间变量的梯度值我们也可以通过后向传播得到吗？从原理上来讲，后向传播可以得到计算图中所有后代节点的梯度值，当然也包括中间变量了。不过处于优化的考虑，Torch 是不会缓存中间变量的梯度值的。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = torch.sum(y)\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  # leaf node has gradient\n",
    "print(y.grad)  # non-leaf node doesn't have gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要访问这些中间变量的梯度值，可以通过 hook 来实现（[hook 介绍](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.register_hook)），示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = x**2\n",
    "z = torch.sum(y)\n",
    "\n",
    "hy = y.register_hook(save_grad('y'))  # return a handle for hook\n",
    "hz = z.register_hook(save_grad('z'))  # return a handle for hook\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(x.grad, grads['y'], grads['z'])\n",
    "\n",
    "# remove hook\n",
    "hy.remove()\n",
    "hz.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hook 将在变量的梯度值每次被计算时调用，hook 的接口：`hook(grad) -> Tensor or None`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了通过 hook 外，我们还可以设置强制缓存非叶节点的梯度值（[详见](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.retain_grad)）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = x**2\n",
    "y.retain_grad()  # 设置保留非叶节点的梯度\n",
    "z = torch.sum(y)\n",
    "\n",
    "z.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度值是累加的**\n",
    "\n",
    "什么是梯度的累加？\n",
    "\n",
    "例如：变量 `x` 既是关于变量 `y` 的自变量，又是关于 `z` 的自变量，那么 `y.backward()` 调用后会得到 `x` 相对于 `y` 的梯度值并保存于 `x.grad` 中，再调用 `z.backward()` 计算 `x` 相对于 `z` 的梯度值后，`x.grad` 中保存的则是前后两次计算梯度值的累加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x\n",
    "z = x\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch 为什么要设计成梯度累加的？\n",
    "\n",
    "变量的梯度可能从不同地方流入的，根据链式求导法则有：\n",
    "\n",
    "$$\\frac {\\partial y} {\\partial x} = \\sum_i \\frac {\\partial y} {\\partial u_i} \\frac {\\partial u_i} {\\partial x}$$\n",
    "\n",
    "其中每个 $u_i$ 都会将梯度流向 $x$。在后向传播时，这些梯度会在不同时间到达 $x$，因此需要累加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**计算图销毁**\n",
    "\n",
    "之前提到，Torch 为了高效的利用内存，会在合适的时机，自动释放计算图，准确来讲每次完成后向传播，计算图就会被自动销毁。这是由于，一般情况下，后向传播完成，梯度已经计算好，再保留计算图就没有意义，因此 Torch 默认会在此时销毁计算图。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "b = a ** 2\n",
    "c = torch.sum(b)\n",
    "\n",
    "# backward is ok\n",
    "c.backward()\n",
    "try:\n",
    "    # backward again, will raise exception\n",
    "    c.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据异常的错误提示，要想要后向传播过后，计算图依然保留，可以传递参数 `retain_graph=True`，这样下次再调用 `backward` 时就不会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "b = a ** 2\n",
    "c = torch.sum(b)\n",
    "\n",
    "# backward is ok\n",
    "c.backward(retain_graph=True)\n",
    "# backward is ok too\n",
    "c.backward()\n",
    "# backward will raise exception, unless provide retain_graph=True in last call backward()\n",
    "# c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看一个稍复杂的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "b = a ** 2\n",
    "c = torch.sum(b)\n",
    "d = torch.mean(b)\n",
    "\n",
    "# backward is ok\n",
    "c.backward()\n",
    "try:\n",
    "    # backward will raise exception\n",
    "    d.backward()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里可以认为 `c` 和 `d` 是同一个计算图中的两个根节点，通过其中任意一个后向传播，则计算图就会被销毁。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看神经网络模型中计算图的构建和销毁："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "linear = torch.nn.Linear(5, 5)\n",
    "for i in range(10):\n",
    "    x = torch.randn(5)\n",
    "    # 每次 forward 动态构建计算图\n",
    "    out = linear(x)\n",
    "    loss = torch.sum(out)\n",
    "    # 每次 backward 后销毁计算图\n",
    "    loss.backward()\n",
    "    # 再次 backward 就会报错了\n",
    "    # loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见每次 forward 都会自动构建计算图，然后在 backward 时被销毁，所以一般我们在迭代数据训练神经网络时，会调用 `backward()` 多次，但也不会出错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**停止自动微分**\n",
    "\n",
    "停止记录作用于某个 Tensor 上的计算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ThAddBackward object at 0x0000000008696C88> True\n",
      "None False\n",
      "tensor(-1., grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "out = x + y\n",
    "print(out.grad_fn, out.requires_grad)\n",
    "a = out.detach()  # 返回一个 Tensor 跟原 Tensor 共享数据储存，但以前的计算历史没有保留\n",
    "print(a.grad_fn, a.requires_grad)  # a 的 grad_fn 属性为 None，没有了计算历史记录\n",
    "\n",
    "a[0] = -1.0  # 新 Tensor 跟原来 Tensor 共享数据储存\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：通过上例可见，变量只是脱离了计算图，无法追溯以前的计算历史记录而已，数据储存还跟以前是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "停止记录代码块中的所有 Tensor 的计算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x+1).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过访问 Tensor 的 `data` 属性可以避免计算操作被记录，属性 `data` 保存了跟原来 Tensor 一样的数据且共享内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "# x = x ** 2\n",
    "x.data = x.data ** 2\n",
    "\n",
    "y = x.sum()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**detach VS. data**\n",
    "\n",
    "在实际用中更加推荐使用 `detach()` 而不是 `.data` 来返回一个不被计算图记录的 Tensor 引用。这是因为，如果修改了返回的 Tensor，则原 Tensor 也会发生变化，若后向传播计算梯度时又用到原来的 Tensor，则可能会出现 bug。使用 `detach()` 的好处在于，在遇到这种情况时，后向传播会报错，而 `.data` 则不会。\n",
    "\n",
    "摘自官方的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 out.detach() 返回 c，若 c 改变引起 out 改变，则后向传播时会报错\n",
    "a = torch.tensor([1,2,3.], requires_grad = True)\n",
    "out = a.sigmoid()\n",
    "c = out.detach()\n",
    "\n",
    "c.zero_()  # 同时改变了 c 和 out\n",
    "\n",
    "try:\n",
    "    out.sum().backward()  # 报错\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "out = a.sigmoid()\n",
    "c = out.data\n",
    "\n",
    "c.zero_()  # 同时改变 c 和 out\n",
    "\n",
    "out.sum().backward()  # 不报错，但梯度 a.grad 是错误的\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自定义自动微分运算操作**\n",
    "\n",
    "支持自动微分的运算操作，其实是由一对函数组成的，其中 forward 函数根据输入变量计算输出变量，backward 函数则根据输出变量相对于某个标量的梯度值来计算输入变量相对于该标量的梯度值。\n",
    "\n",
    "不论是 Tensor 重载的运算符，还是 `torch.sum`, `torch.sigmoid` 等各种运算操作，本质上都是由 forward 和 backward 函数构成的而已。要自定义自动微分运算操作，需要继承 `torch.autograd.Function` 父类，并实现这两个函数。\n",
    "\n",
    "摘自官网的示例，定义以及使用 ReLU 运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 定义运算操作\n",
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "# 使用该运算操作\n",
    "a = torch.tensor([1., -1., 2.], requires_grad=True)\n",
    "b = MyReLU.apply(a)\n",
    "b.sum().backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**固定模型的部分**\n",
    "\n",
    "通过模型一部分的变量设置为 `requires_grad=False`，可以使得利用后向传播时，这部分变量不会被计算梯度\n",
    "\n",
    "\n",
    "如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "\n",
    "z = x + y  # requires_grad is False\n",
    "print(z.requires_grad)\n",
    "\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "\n",
    "z = z + a\n",
    "print(z.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "通过 Tensor 和自动微分理论上可以实现任何神经网络模型，但手动构建模型，指定输入、参数、输出以及损失函数等，较为麻烦。Torch 的子库 `nn` 提供了更高的抽象结构，可以用来快速构建神经网络模型，并且所有运算操作都是基于自动微分技术的。\n",
    "\n",
    "\n",
    "## 神经网络实例\n",
    "\n",
    "![](mnist.png)\n",
    "\n",
    "以上是一个用来进行手写数字识别的卷积神经网络（CNN）结构：输入图片是尺寸为 32x32 的单通道图片；卷积层 C1 有 6 个 28x28 的特征图（也即感知域大小为 5x5，移动步长为 1）；子采样层 S2 有 6 个 14x14 的子图（即子采样区域大小为 2x2）。卷积层 C3 的输入是大小为 14x14 的子采样图且通道为 6，该层有 16 个 10x10 的特征图（也即感知域大小为 5x5，移动步长为 1）；子采样层 S4 有 16 个 5x5 的子图（即子采样区域大小为 2x2）；接下来的全连接层 C5 含有 120 个隐单元；全连接层 F6 含有 84 个隐单元；输出层含有 10 个神经元（跟 10 个阿拉伯数字相对应）。\n",
    "\n",
    "用 Torch 来对该 CNN 建模如下（参照[官方文档](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)，略做改动）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first conv layer: \n",
    "        # input 1-channel image, output 6-channel feature maps.\n",
    "        # convolution kernel size is 5x5. gap is 1.\n",
    "        self.conv1 = nn.Conv2d(1, 6, (5, 5))\n",
    "        # second conv layer:\n",
    "        # input 6-channel feature maps, output 16 feature maps.\n",
    "        # convolution kernel size is 5x5. gap is 1.\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n",
    "        # Full connection layers:\n",
    "        # the last convolution layer output 16 feature maps of 5x5 size\n",
    "        feature_num = 16*5*5\n",
    "        self.fc1 = nn.Linear(feature_num, 120)  # affine transform\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"`x` is a mini-batch of samples, not a single sample.\"\"\"\n",
    "        # max pooling window size is 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(x.size()[0], -1) # flatten feature maps\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的学习框架\n",
    "\n",
    "基于神经网络的机器学习算法，一般具有如下学习流程：\n",
    "\n",
    "1. 根据神经网络的结构，来创建模型（含参）\n",
    "2. 迭代训练数据来学习模型参数，数据一般以 mini-batch 为单位进行迭代，不断重复下面的几个步骤：\n",
    "\n",
    "    1. Forward：数据从神经网络输入端流向输出端，以计算输出值\n",
    "    2. Loss：计算输出值和真实值之间的误差值，以及输出值的误差梯度\n",
    "    3. Backward：误差梯度从神经网络的输出端流向输入端，以计算网络参数的梯度值\n",
    "    4. Update Parameters：利用参数的梯度信息，采取特定策略来更新参数值，以减小网络的误差值\n",
    "\n",
    "3. 利用测试数据评估模型性能（也有可能会在每次处理完一个 mini-batch 时进行）\n",
    "\n",
    "\n",
    "利用该流程，处理上例中的手写数字识别问题，伪代码如下：\n",
    "\n",
    "    epoch = 100\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    train_data, test_data = load_data()\n",
    "    \n",
    "    net = Network()\n",
    "    loss_fn = MeanSquaredError()\n",
    "    optimizer = SGD(net.parameters(), learning_rate)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        # 每次迭代训练集时先将其随机打乱\n",
    "        train_data = random_shuffle(train_data)\n",
    "        # 迭代处理 mini-batch\n",
    "        for mini_batch in train_data:\n",
    "            inputs, targets = mini_batch\n",
    "            # Forward\n",
    "            outputs = net.forward(inputs)\n",
    "            # Loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            # Backward\n",
    "            optimizer.zero_grad()  # 反向传播之前先清空梯度\n",
    "            loss.backward()\n",
    "            # Update Parameters\n",
    "            optimizer.step()\n",
    "        # 评估模型性能\n",
    "        test_inputs, test_targets = test_data[:,0], test_data[:,1]\n",
    "        loss = loss_fn(net.forward(test_inputs), test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的抽象结构\n",
    "\n",
    "* 网络层：网络层是神金网络的基本结构，包含若干神经元和可调节的参数，数据输入并流出网络层。Torch 提供了 `torch.nn.Module` 用来表示网络层。\n",
    "* 损失函数：不同模型需要使用不同的损失函数来度量预测值和真实值之间的差异。`torch.nn` 提供了许多常见的损失函数。\n",
    "* 优化方法：神经网络模型一般通过梯度信息来调节网络中的参数，`torch.optim` 提供了不同的调节机制。\n",
    "\n",
    "\n",
    "一个隐含层的全连接网络模型，示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n = 100\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "input_dim, hidden_dim, output_dim = 10, 66, 5\n",
    "\n",
    "# 定义模型：使用 torch.nn.Sequential 将多个 Module 按顺序连接起来\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, hidden_dim),  # affine layer 含有可调节参数 weights 和 bias 等\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_dim, output_dim)\n",
    ")\n",
    "# 定义损失函数：Mean Squared Error\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "# 定义优化方法：SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 构造训练数据\n",
    "x = torch.randn(n, input_dim)\n",
    "y = torch.randn(n, output_dim)\n",
    "\n",
    "# 开始迭代训练\n",
    "for e in range(epoch):\n",
    "    t = model(x)\n",
    "    loss = loss_fn(t, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上例可见，通过对网络层、损失函数、优化算法的抽象封装后。如何定义网络细节结构、如何定义可调节参数、如何在网络中后向传播、如何利用梯度信息更新网络参数等，这些逻辑都变得透明了。这就是抽象神经网络结构的好处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义网络层\n",
    "\n",
    "可以通过集成 `torch.nn.Module` 父类来自定义网络层，子类需要实现从输入映射到输出的逻辑，至于梯度的后向传播逻辑则由 Tensor 的自动微分来实现。\n",
    "\n",
    "自定义含有一个 sigmoid 隐含层的网络层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NetSigmoid(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x is a mini-batch of sample.\"\"\"\n",
    "        return self.fc2(self.fc1(x).sigmoid())\n",
    "\n",
    "net = NetSigmoid(10, 20, 5)\n",
    "x = torch.randn(1, 10)\n",
    "print(net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "\n",
    "\n",
    "机器学习算法是由数据驱动的算法，因此数据处理是每个机器学习算法必然会面对的问题。\n",
    "\n",
    "常见的数据处理问题有：各种音视频以及文本等数据的加载、存储和格式变换；数据的规范化处理；数据 Augment；数据可视化；特征工程等等。\n",
    "\n",
    "我们可以利用各种现成的库和工具来进行数据处理，如：\n",
    "\n",
    "* 用 Pillow 和 OpenCV 处理图片和视频数据\n",
    "* 用 scipy 和 librosa 处理音频数据\n",
    "* 用 NLTK 和 SpaCy 处理文本数据\n",
    "* PyTorch 组开发的 `torchvision` 可以处理常见的计算机视觉数据集\n",
    "\n",
    "## Dateset\n",
    "\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "__len__ so that len(dataset) returns the size of the dataset.\n",
    "__getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "Let’s create a dataset class for our face landmarks dataset. We will read the csv in __init__ but leave the reading of images to __getitem__. This is memory efficient because all the images are not stored in the memory at once but read as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, n, transform=None):\n",
    "        self.data = range(n)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.data[i]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# 迭代数据集\n",
    "d = SimpleDataset(5, lambda n: n ** 2)\n",
    "for i in range(len(d)):\n",
    "    print(d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "Data format transform or augement.\n",
    "\n",
    "由上面的例子可知，transform 仅仅支持一个函数，用来对数据集中的样本进行处理。\n",
    "\n",
    "这里我们用工厂方法来实现 transform 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowTransform(object):\n",
    "    \n",
    "    def __init__(self, power=2):\n",
    "        self.power = power\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        return sample ** self.power\n",
    "\n",
    "# 迭代数据集\n",
    "pt = PowTransform(3)\n",
    "d = SimpleDataset(5, pt)\n",
    "for i in range(len(d)):\n",
    "    print(d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组合多个 transforms，利用 torchvision.transforms.Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "直接通过 for-in 迭代数据，不能实现以下功能：\n",
    "\n",
    "Batching the data\n",
    "Shuffling the data\n",
    "Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "不过可以使用 dataloader 来实现：\n",
    "\n",
    "torch.utils.data.DataLoader is an iterator which provides all these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pt = PowTransform(3)\n",
    "d = SimpleDataset(50, pt)\n",
    "print(d)\n",
    "dataloader = DataLoader(d, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "print(dataloader)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
