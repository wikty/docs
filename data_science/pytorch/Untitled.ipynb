{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "PyTorch 是基于 Python 的科学计算包，主要有两大特色：\n",
    "\n",
    "* 用于多维数组的计算，可以作为 NumPy 的替代品。PyTorch 具有 GPU 加速的优势\n",
    "* 提供了许多深度学习方面的开发工具，如神经网络库、优化算法、自动微分等\n",
    "\n",
    "# Tensor\n",
    "\n",
    "PyTorch 中主要的计算对象是 Tensor，就如果 NumPy 中的 ndarray 一样。不过所有基于 Tensor 的计算，都在背后使用了 GPU 加速带来的性能优势。\n",
    "\n",
    "**Tensor 创建**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 2.8594, -2.8354, -1.3083, -0.4523,  0.2130],\n",
      "        [ 1.0412,  1.3450,  1.8498,  1.5332,  1.5354],\n",
      "        [ 0.6882,  1.1864, -2.0713, -0.0863, -0.0491]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([[ 0.4382,  3.0420, -1.5897,  0.0575,  0.3343],\n",
      "        [-1.1480,  0.6118, -0.9391, -0.8428,  0.9792],\n",
      "        [-0.9095, -2.0430,  1.5645,  0.9263, -0.1956]])\n",
      "tensor([[ 0.4230, -0.0119, -0.6443,  0.1949, -1.0253],\n",
      "        [ 1.0984, -1.4942,  0.7026,  1.2779, -0.1952],\n",
      "        [-0.8727, -0.3499, -0.8838, -1.6389, -0.4629]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.tensor([[1, 2], [3, 4]])\n",
    "print(x)\n",
    "print(x.size())\n",
    "\n",
    "x = th.empty(3, 5)\n",
    "print(x)\n",
    "\n",
    "x = th.zeros(3, 5)\n",
    "print(x)\n",
    "\n",
    "x = th.ones(3, 5, dtype=th.long)\n",
    "print(x)\n",
    "\n",
    "y = th.ones_like(x)\n",
    "print(y)\n",
    "\n",
    "x = th.randn(3, 5)\n",
    "print(x)\n",
    "\n",
    "y = th.randn_like(x, dtype=th.double)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 运算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7873, 0.4587],\n",
      "        [0.1525, 0.6062]])\n",
      "tensor([[0.5259, 0.3414],\n",
      "        [0.8885, 0.9400]])\n",
      "tensor([[1.3132, 0.8001],\n",
      "        [1.0409, 1.5462]])\n",
      "tensor([[1.3132, 0.8001],\n",
      "        [1.0409, 1.5462]])\n",
      "tensor([[1.3132, 0.8001],\n",
      "        [1.0409, 1.5462]])\n",
      "tensor([[1.3132, 0.8001],\n",
      "        [1.0409, 1.5462]])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.rand(2, 2)\n",
    "y = th.rand(2, 2)\n",
    "z = th.empty(2, 2)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "z = th.add(x, y)\n",
    "print(z)\n",
    "\n",
    "th.add(x, y, out=z)\n",
    "print(z)\n",
    "\n",
    "y.add_(x)  # add x to y in-place\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 索引**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3]])\n",
      "tensor([[1]])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "x = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(x[0, 0])\n",
    "print(x[0, :])\n",
    "print(x[0:1, :])\n",
    "print(x[0:1, 0:1])\n",
    "print(x[0:1, 0:1].item())  # if the tensor just have one number, item() can return it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 重塑**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change shape\n",
      "torch.Size([4, 4]) torch.Size([2, 8])\n",
      "add new axis\n",
      "torch.Size([4, 4]) torch.Size([1, 4, 4]) torch.Size([1, 4, 4])\n",
      "torch.Size([4, 4]) torch.Size([1, 4, 4]) torch.Size([4, 1, 4])\n",
      "torch.Size([4, 4]) torch.Size([1, 4, 4]) torch.Size([4, 1, 4])\n",
      "remove axis\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([4, 1, 4]) torch.Size([4, 4]) torch.Size([4, 4]) torch.Size([4, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"change shape\")\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size())\n",
    "\n",
    "print(\"add new axis\")\n",
    "y = x.view(-1, 4, 4)\n",
    "z = x.view(1, 4, 4)\n",
    "print(x.size(), y.size(), z.size())\n",
    "a = x.unsqueeze(0)\n",
    "b = x.unsqueeze(1)\n",
    "print(x.size(), a.size(), b.size())\n",
    "a = x[None]\n",
    "b = x[:, None]\n",
    "print(x.size(), a.size(), b.size())\n",
    "\n",
    "print(\"remove axis\")\n",
    "y = x.view(16)\n",
    "z = x[:,None]\n",
    "a = z.squeeze()  # remove all the dimensions with size 1, removed\n",
    "b = z.squeeze(1)  # remove 1-dim if its size is 1, removed\n",
    "c = z.squeeze(0)  # remove 0-dim if its size is 1, cannot remove\n",
    "print(x.size(), y.size(), z.size(), a.size(), b.size(), c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**处理 NumPy 数组**\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Tensor to NumPy Array\n",
      "tensor([1., 1., 1.]) [1. 1. 1.]\n",
      "[2. 2. 2.]\n",
      "NumPy Array to Torch Tensor\n",
      "[1. 1. 1.] tensor([1., 1., 1.], dtype=torch.float64)\n",
      "tensor([2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "print(\"Torch Tensor to NumPy Array\")\n",
    "\n",
    "a = torch.ones(3)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a.add_(1)  # change a, will change b\n",
    "print(b)\n",
    "\n",
    "print(\"NumPy Array to Torch Tensor\")\n",
    "c = numpy.ones(3)\n",
    "d = torch.from_numpy(c)\n",
    "print(c, d)\n",
    "\n",
    "numpy.add(c, 1, out=c)  # change c, will change d\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor 和 CUDA**\n",
    "\n",
    "Torch 中的 Tensor 支持利用 CUDA 进行计算，可以自由的在 CUDA 和 CPU 计算设备之间进行切换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2647,  2.5366,  1.7845,  0.8997],\n",
      "        [ 2.3217,  1.9958,  1.0311,  0.3865],\n",
      "        [ 0.5902, -0.3435,  1.2320,  0.3471],\n",
      "        [ 1.7282,  0.7726,  1.7006,  1.2985]], device='cuda:0')\n",
      "tensor([[-0.2647,  2.5366,  1.7845,  0.8997],\n",
      "        [ 2.3217,  1.9958,  1.0311,  0.3865],\n",
      "        [ 0.5902, -0.3435,  1.2320,  0.3471],\n",
      "        [ 1.7282,  0.7726,  1.7006,  1.2985]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "else:\n",
    "    print(\"CUDA isn't available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoDiff\n",
    "\n",
    "自动微分技术是现代深度学习框架的一大特色，MXNet,TensorFlow 等框架都提供了自动微分工具，使得梯度的求解对于机器学习开发者变得透明。同样 Torch 也支持自动微分，通过子库 `autograd` 提供的自动微分技术，使得一切基于 Tensor 的运算操作都可以进行自动微分。也即在开发神经网络模型时，只要定义了模型结构，就也意味着定义了梯度求解方法，后向传播计算会自动进行。\n",
    "\n",
    "Torch 中自动微分的实现是通过记录追踪计算对象 Tensor 和计算操作 Function 对象来实现的，Tensor 对象通过属性 `grad_fn` 来引用生成它计算操作 Function 对象，Tensor 和 Function 对象相互连接构成一个计算图，代表了模型中完整的计算历史过程，利用该计算历史即可实现自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 声明某个 Tensor 上的计算操作要被记录："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = torch.ones(2, 2)\n",
    "print(y)\n",
    "y.requires_grad_(True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor 引用生成它的 Function 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward1 object at 0x0000000008D01F28> <AddBackward object at 0x0000000008D01B38> None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "y = x + 1\n",
    "z = y.mean()\n",
    "\n",
    "print(z.grad_fn, y.grad_fn, x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 作用于某个 Tensor 上当前计算操作所对应的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-972.2409, -720.7725, -379.6190], grad_fn=<MulBackward>)\n",
      "tensor([ 102.4000, 1024.0000,    0.1024])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)  # 参数 gradients 的 shape 应该跟 y 保持一致\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 停止记录将来作用于某个 Tensor 上的计算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "x.detach()\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 停止记录代码块中的所有 Tensor 的计算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Torch 提供了子库 `nn` 可以用来快速构建神经网络模型，并且所有运算操作都是基于自动微分技术的。\n",
    "\n",
    "\n",
    "\n",
    "## 神经网络实例\n",
    "\n",
    "![](mnist.png)\n",
    "\n",
    "以上是一个用来进行手写数字识别的卷积神经网络（CNN）结构：输入图片是尺寸为 32x32 的单通道图片；卷积层 C1 有 6 个 28x28 的特征图（也即感知域大小为 5x5，移动步长为 1）；子采样层 S2 有 6 个 14x14 的子图（即子采样区域大小为 2x2）。卷积层 C3 的输入是大小为 14x14 的子采样图且通道为 6，该层有 16 个 10x10 的特征图（也即感知域大小为 5x5，移动步长为 1）；子采样层 S4 有 16 个 5x5 的子图（即子采样区域大小为 2x2）；接下来的全连接层 C5 含有 120 个隐单元；全连接层 F6 含有 84 个隐单元；输出层含有 10 个神经元（跟 10 个阿拉伯数字相对应）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch 对该 CNN 建模如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0677, -1.0059, -0.1112],\n",
      "        [ 0.9542,  1.0518,  0.4728]])\n",
      "tensor([[[ 0.0677, -1.0059, -0.1112],\n",
      "         [ 0.9542,  1.0518,  0.4728]]])\n",
      "tensor([[ 0.0677, -1.0059, -0.1112],\n",
      "        [ 0.9542,  1.0518,  0.4728]])\n",
      "(tensor([0.9542, 1.0518, 0.4728]), tensor([1, 1, 1]))\n",
      "(tensor([0.0677, 1.0518]), tensor([0, 1]))\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "print(x)\n",
    "\n",
    "print(x.unsqueeze(0))\n",
    "print(x.data)\n",
    "print(torch.max(x.data, 0))\n",
    "print(torch.max(x.data, 1))\n",
    "\n",
    "print(x==x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
